---
title: "Linear Regression from Scratch"
author: "Braescu Rares & Bujor Alexandru"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Linear Regression

Also known as **univariate linear regression** or **simple linear regression**, are used to predict a single output from a single input. This technique is part of a bigger category named ***Supervised Learning***, where data is labeled and the output values are known in the training data.

### 1.1. Hypothesis Function

A **linear regression** with many variables *x* is known as **multivariate linear regression** or **multiple linear regression**. The *hypothesis function* for linear regression can then be written as 

$$h_w(x) = \sum_{j=0}^{n} w_j x_j + \epsilon$$

### 1.2. Cost Function

Having many possible values $w_0, w_1, ..., w_n$ for which we have many possible regression lines, we would like to find the regression line which minimizes the cost function, which in the case of regression is the ***residual sum of squares***(RSS).

$$
\begin{aligned}
RSS(w) &= \sum_{i=0}^{m} (y^{(i)} - (\sum_{j=0}^{n} w_j x_j^{(i)}))^2 \\
         &= \frac{1}{2m} \sum_{i=0}^{m} (y^{(i)} - h_w(x^{(i)}))^2 \\ 
         &= (y - \boldsymbol{Hw})^T(y - \boldsymbol{Hw}) 
\end{aligned}
$$

## 2. Linear Regression as Ordinary Least Squares

```{r, echo=FALSE}
height <- c(176, 154, 138, 196, 132, 176, 181, 169, 150, 175)
bodymass <- c(82, 49, 53, 112, 47, 69, 77, 71, 62, 78)
plot(bodymass, height, pch = 16, cex = 1.3, col = "blue", main = "Linear Regression Fit Example", xlab = "x", ylab = "y")
segments(69, 176, 69, 163.7472)
points(69, 163.7472, col="green", cex = 1.5, pch = 16, )
abline(lm(height ~ bodymass))
```

In the plot above, the fit is calculated by minimizing the sum of squared errors. If $\hat{y}_i = \hat{w}_0 + \hat{w}_1 x_i$, then $e_i = y_i - \hat{y}_i$ is the error for the *i*th observation. For all observations, we can write 
$$
\begin{align}
RSS &= e^2_1 + e^2_2 + ... + e^2_m \\
    &= \sum_{i=1}^{m} (y_i - \hat{w}_0 - \hat{w}_1 x_i)^2
\end{align}
$$

In order to minimize RSS we will diferentiate with respect to $w_0$ and $w_1$, respectively :
$$
\begin{align}
\frac{\partial{RSS}}{\partial{w_0}} &= \frac{\partial}{\partial{w_0}}\sum_{i=1}^{m}(y_i - \hat{w}_0 - \hat{w}_1x_i)^2\\ &= -2 \sum_{i=1}^{m}(y_1 - w_0 - w_1x_i)
\end{align}
$$
and 
$$
\begin{align}
\frac{\partial{RSS}}{\partial{w_1}} &= \frac{\partial}{\partial{w_1}}\sum_{i=1}^{m}(y_i - \hat{w}_0 - \hat{w}_1x_i)^2\\ &= -2 \sum_{i=1}^{m}x_i(y_1 - w_0 - w_1x_i)
\end{align}
$$
Equating the above equations to zero gives us:

$$
\begin{align}
  w_0 &= \frac{\sum_{i=1}^{m}y_i}{m} - w_1\frac{\sum_{i=1}{m}x_i}{m} \\
  &= \bar{y} - w_1\bar{x} \end{align} 
$$
$$
  0 = \sum_{i=1}^{m}y_ix_i - \sum_{i=1}^{m}w_0x_i - \sum_{i=1}^{m}w_1x_i^2
$$
Substituting the value of $w_0$ from the first result will result in :

$$
w_1 = \frac{\sum_{i=1}^{m}y_ix_i - \frac{\sum_{i=1}^{m}y_i\sum_{i=1}^{m}x_i}{m}}{\sum_{i=1}^{m}x_i^2 - \frac{(\sum_{i=1}^{m}x_i)^2}{m}}
$$
In matrix form the equations for $w_0$ and $w_1$ can be combined to arrie at the ordinary least squares (OLS)  regression equation :
$$
 w = (X^\top X)^{-1}X^\top Y
$$

## 3. Linear Regression as Maximum Likelihood

Linear regression can be justified as a maximum likelihood procedure. The estimation of weights as we calculate earlier was to choose a w which would minimize the Mean Squared Error(MSE).

We now look again the linear regression from the point of view of maximum likelihood estimation.
To derive the linear regression algorithm, we define:
$$
p(y | x) = \mathcal{N}(y | \hat y(x,w), \sigma^2)
$$

The function $\hat y(x,w)$ gives the prediction of the mean of the Gaussian.

We can write the likelihood of the RSS equation as the product of the probabilities :

$$
\begin{align}
 \mathcal{L}(p(y | x)) &= \prod_{i=1}^{m} \sqrt{(2\pi\sigma^2)}exp(-\frac{1}{2}\frac{(y_i-wx_i)^2}{\sigma^2}) \\
 &= (2\pi\sigma^2)^{m/2}exp(-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i - wx_i)^2)
 \end{align}
$$

### 4.3 Step-by-Step Batch Gradient Descent

In this procedure, we update the weights with every input

$$
while\ ||\frac{\partial}{\partial w} RSS(w^{(t)})|| > \nu  \\ 
for\ features\ in\ j = 0, 1, ..., n \\
\frac{\partial}{\partial w} RSS(w^{(t)}_j) = - \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - h_w(x_j^{(i)})) (x_j^{(t)}) \\
\begin{align}
w_j^{(t+1)} &\leftarrow w_j^{(t)} - \eta \frac{\partial}{\partial w} RSS(w_j^{(t)}) \\
w_j^{(t+1)} &\leftarrow w_j^{(t)} + 2\eta H^{\top}(y - Hw^{(t)}) \\
\eta\ &is\ the\ step\ size
\end{align}
$$

The norm of the gradient of the cost is what we try to minimize, by a defined *tolerance* value and is defined as

$$
||\frac{\partial}{\partial w} RSS(w^{(t)})|| = \sqrt{\frac{\partial}{\partial w}RSS(w_0^{(t)})^2 + \frac{\partial}{\partial w}RSS(w_1^{(t)})^2 + ... + \frac{\partial}{\partial w}RSS(w_m^{(t)})^2}
$$

### 4.4 Writing the Batch Gradient Descent Application

The *data_matrix* function accepts the selected features and adds the constant column of "ones" and scales the features before returning as a matrix. Scaling the data ensures faster convergence during gradient descent.

```{r, eval=FALSE}
data_matrix <- function(data, features, output){
  scaled_feature_matrix <- data.frame(scale(data[features]), row.names=NULL)
  length <- nrow(data)
  scaled_features <- as.matrix(cbind('Intercept' = rep(1, length),
                                    scaled_feature_matrix[, features]
                                    )
                              )
  output <- as.matrix(scale(data[output]))
  return(list(scaled_features, output))
}
```

The *predict_output* function accepts the feature data and a list of "weights" corresponding to each feature and returns the predicted values.

```{r, eval=FALSE}
predict_output = function(feature_matrix, weights) {
  predictions = (as.matrix(feature_matrix)) %*% weights
  return(predictions)
}
```

The *featureDerivative* function calculates the gradient of the RSS at each feature.

```{r, eval=FALSE}
featureDerivative = function(errors, features) {
  derivative = -1/nrow(features) * (t(features) %*% errors)
  return(derivative)
}
```

This function performs a step-by-step gradient descent and converges if the square
root of the squared sums of each of the partial gradients is less than the
defined tolerance. It returns the computed weights for each feature and the number
of iterations it took to converge.

```{r, eval=FALSE}
regression_gradient_descent = function(feature_matrix, output,
  initial_weights, step_size, tolerance) {
  converged = FALSE
  weights = initial_weights
  i = 0
  while (!converged) {
    predictions = predict_output(feature_matrix, weights)
    errors = predictions - output
    gradient = featureDerivative(errors, feature_matrix)
    gradient_norm = sqrt(sum(gradient^2))
    weights = weights + step_size * gradient
    if (gradient_norm < tolerance) {
      converged = TRUE
    }
    i = i + 1
  }
  return(list(weights = weights, Iters = i))
}
```

Assuming there is a dataset that contains *price*, *bedrooms* and *sqft_linving* information about houses.

```{r, eval=FALSE}
kc_house_data <- read.csv(paste(file_path, "kc_house_data.csv", sep = ""))
house_data <- kc_house_data[, c("price", "bedrooms", "sqft_living")]
```

The next step is to partition the data set into training and test data sets.

```{r, eval=FALSE}
set.seed(22)
inTrain <- createDataPartition(house_data$price, p = 0.5, list = F)

house_train_data = house_data[inTrain,]
house_test_data = house_data[-inTrain,]
```

Now, let's define the *features* and the *respone*.

```{r, eval=FALSE}
my_features <- c("bedrooms", "sqft_living")
my_output <- "price"
```

Construct the feature matrix, the response matrix and set the initial weights, step size and tolerence.

```{r, eval=FALSE}
feature_matrix = data_matrix(house_train_data, my_features, my_output)[[1]]
output_matrix = data_matrix(house_train_data, my_features, my_output)[[2]]

initial_weights = c(0, 0, 0)
step_size = 0.01
tolerance = 1e-5
```

Finally, the gradient descent algorithm described above returns the feature weigths.

```{r, eval=FALSE}
weights = regression_gradient_descent(feature_matrix, output_matrix, initial_weights, step_size, tolerance)
```

We can also cross-check obtained values with OLS regression in R.

```{r, eval=FALSE}
scaled_house_data <- data.frame(scale(house_train_data))
lm(price ~ ., data = scaled_house_data)$coef
```

To ease our lives, R has *gdescent* function that can be found in the **gettingtothebottom** package. It allows us to define the objective function *f*, the gradient of the objective function *grad_f*, feature matrix *X*, the response vector *y*, the step size *alpha*, the number of iterations *iter* and the tolerance.

```{r, eval=FALSE}
library(gettingtothebottom)
X <- as.matrix(feature_matrix[, -1])
y <- as.vector(output_matrix)

# initialise the weights
b <- c(0, 0, 0)

# define the objective function
f <- function(X, y, b) {
  (1/2) * norm(y - X %*% b, "F")^2
}

# Calculate gradient of the objective function
grad_f <- function(X, y, b) {
  t(X) %*% (X %*% b - y)
}

gradient_descent <- gdescent(f, grad_f, X, y, alpha = 10e-5, iter = 10e7, tol = 1e-5)
```

After running code from above, it turns out we get the same results as with our linear regression algorithm. Convergence depends on the values of the step size and the tolerance. A higher value of step size may lead to non-convergence and a lowert tolerance value might take a long time to converge. A general thumb rule is to start with a higher step size and a low convergence criterion and keep adjusting them.

In the **Batch Gradient Descent** algorithm described above, the algorithm sums up the gradient terms over all the *m* cases in the data before modifying the parameters. For a large size data set say, $m=1e20$, this would take a long time to converge. In such cases, stochastic gradient descent is a very good option.

### 4.5 Writing the Stochastic Gradient Descent Application

In stochastic gradient descent, the algorithm selects a single observation at random (instead of sweeping through all the cases, in batch gradient descent), from the data set and starts modifying the parameters right away. This process is repeated and the weights modified, till it has worked through the entire data set. It then repeats the above procedure for the number of iterations defined.

```{r, eval=FALSE}
sgd <- function(x, y, betas, iters, lambda) {
  beta<-as.matrix(cbind(rep(betas[1], iters),
                        rep(betas[2], iters),
                        rep(betas[3],iters))
                  )
  for (i in 2:iters) {
    m <- nrow(x)
    sample_num <- sample.int(m, 1)
    row_x <- x[sample_num,]
    row_y <- y[sample_num]
    
    beta[i, 1] <- beta[i-1, 1] - (lambda*(beta[i-1, 1] +
                                  beta[i-1, 2]*row_x[[1]] +
                                  beta[i-1, 3]*row_x[[2]] -
                                  row_y)
                                  )
    beta[i, 2] <- beta[i-1, 2] - (lambda*(beta[i-1, 1] +
                                          beta[i-1, 2]*row_x[[1]] +
                                          beta[i-1,3]*row_x[[2]] -
                                          row_y)*row_x[[1]]
                                  )
    beta[i, 3] <- beta[i-1,3] - lambda*(beta[i-1, 1] +
                                        beta[i-1, 2]*row_x[[1]] +
                                        beta[i-1,3]*row_x[[2]] -
                                        row_y)*row_x[[2]]
  }
  
  return(beta);
}
```

Let's apply the above function to our *feature_matrix* and compare the results with the standard multiple regression function in R

```{r, eval=FALSE}
betas = c(0, 0, 0)
iters = 3000
lambda = 0.0001
weights = sgd(feature_matrix[, -1],
              output_matrix,
              betas,
              iters = iters,
              lambda = lambda
              )
```

|      |Intercept   |sqft_living |Bedrooms |
|:----:|:----------:|:----------:|:-------:|
|SGD   |0.0028      |0.0704      |0.1802   |
|lm    |0.0000      |-0.1490     |0.7885   |

It may be observed that the parameters outputs from a SGD are not exactly the same as the regression equation output but it is very close. In fact, SGD hovers around local minima without actually converging. For most practical purposes, this suffices when we are using large data sets.

## 7 Ridge Regression

Adding higher degree polynomials to a regression equation led to overfitting. Overfitting occurs when the model fits the training data all too well and does not generalize to unseed data.

Overfitting can also happen if there are too many predictor variables in the regression equation or, if there are too few observations. Overfitting is also associated with very large estimated parameters (weights) $\hat w$.

Therefore, we want to seek balance between

- How well our model fits the data (measure of fit)
- Magnitude of the coefficients

The total cost of the model is therefore a combination of the measure of fit and the measure of the magnitude of the coefficients. The measure of fit is represented by the *RSS* and a small *RSS* is indicative of a good fit. The measure of magnitude of the coefficients is either the sum of the absolute value of the coefficients $l_1$ norm or, the sum of the squared values of the coefficients $l_2$ norm. They are represented as follows:
$$
||w_0|| + ||w_1|| + ... + ||w_n|| = \sum_{j=0}^{n} ||w_j|| = ||w||_1 (l_1\ Norm)
$$

$$
w_0^2 + w_1^2 + ... + w_n^2 = \sum_{j=0}^{n} w_j^2 = ||w||_2^2 (l2\ Norm)
$$

In ridge regression, we consider the $l_2$ Norm as the measure of the magnitude of the coefficients. The total cost is therefore

$$
Total\ Cost = RSS(w) + ||w||_2^2
$$

Our objective in ridge regression is to find $\hat w$ so as to minimize the total cost of the above formula. The balance between the fit and the magnitude is achieved by introducing a tuning parameter $\lambda$ so that

$$
Total\ Cost = RSS(w) + \lambda||w||_2^2
$$
If $\lambda = 0 \rightarrow$ reduces to minimizing $RSS(w) \rightarrow \hat w^{Least\ Squares(LS)}$.  
If $\lambda = \infty \rightarrow$, total cost is $\infty$ when $(\hat w \neq 0)$ and total cost is 0 when $(\hat w = 0)$.  
If $\lambda$ is in between $\rightarrow 0 \leq ||\hat w||_2^2 \leq ||\hat w^{(LS)}||_2^2$.  

