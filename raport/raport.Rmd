---
title: "Linear Regression from Scratch"
author: "Braescu Rares & Bujor Alexandru"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Linear Regression

Also known as **univariate linear regression** or **simple linear regression**, are used to predict a single output from a single input. This technique is part of a bigger category named ***Supervised Learning***, where data is labeled and the output values are known in the training data.

### 1.1. Hypothesis Function

A **linear regression** with many variables *x* is known as **multivariate linear regression** or **multiple linear regression**. The *hypothesis function* for linear regression can then be written as 

$$h_w(x) = \sum_{j=0}^{n} w_j x_j + \epsilon$$

### 1.2. Cost Function

Having many possible values $w_0, w_1, ..., w_n$ for which we have many possible regression lines, we would like to find the regression line which minimizes the cost function, which in the case of regression is the ***residual sum of squares***(RSS).

$$
\begin{aligned}
RSS(w) &= \sum_{i=0}^{m} (y^{(i)} - (\sum_{j=0}^{n} w_j x_j^{(i)}))^2 \\
         &= \frac{1}{2m} \sum_{i=0}^{m} (y^{(i)} - h_w(x^{(i)}))^2 \\ 
         &= (y - \boldsymbol{Hw})^T(y - \boldsymbol{Hw}) 
\end{aligned}$$

## 2. Linear Regression as Ordinary Least Squares

```{r, echo=FALSE}
height <- c(176, 154, 138, 196, 132, 176, 181, 169, 150, 175)
bodymass <- c(82, 49, 53, 112, 47, 69, 77, 71, 62, 78)
plot(bodymass, height, pch = 16, cex = 1.3, col = "blue", main = "Linear Regression Fit Example", xlab = "x", ylab = "y")
segments(69, 176, 69, 163.7472)
points(69, 163.7472, col="green", cex = 1.5, pch = 16, )
abline(lm(height ~ bodymass))
```

In the plot above, the fit is calculated by minimizing the sum of squared errors. If $\hat{y}_i = \hat{w}_0 + \hat{w}_1 x_i$, then $e_i = y_i - \hat{y}_i$ is the error for the *i*th observation. For all observations, we can write 
$$
\begin{align}
RSS &= e^2_1 + e^2_2 + ... + e^2_m \\
    &= \sum_{i=1}^{m} (y_i - \hat{w}_0 - \hat{w}_1 x_i)^2
\end{align}
$$



